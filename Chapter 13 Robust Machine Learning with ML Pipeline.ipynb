{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71349ff1-3dd6-426e-bdca-3ded736ebb35",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 13 Robust machine learning with ML Pipelines\n",
    "This chapter covers\n",
    "\n",
    "- Using transformers and estimators to transform data into ML features\n",
    "- Assembling features into a vector through an ML pipeline\n",
    "- Training a simple ML model\n",
    "- Evaluating a model using relevant performance metrics\n",
    "- Optimizing a model using cross-validation\n",
    "- Interpreting a modelâ€™s decision-making process through feature weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e6051-8343-4571-b5fa-e93ab780ed21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python Sklearn Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c975ded6-917f-4edc-afb2-bb94f50a00cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 11.288378916846883, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with a scaler and logistic regression model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'classifier__C': np.logspace(-4, 4, 20),\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "clf = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Best Hyperparameters:\", clf.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cd268-8ed1-49eb-a7a0-54cef7423ea9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98dd82f1-09e6-4cc7-8654-463f9f295a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "root_path='/net/clusterhn/home/'+account+'/isa460/Data/'\n",
    "\n",
    "# append path to helper_functions to system path\n",
    "sys.path.append('/net/clusterhn/home/sli/isa460_sli')\n",
    "\n",
    "import helper_functions as H\n",
    "\n",
    "# check if the Spark session is active. If it is activate, close it\n",
    "\n",
    "try:\n",
    "    if spark:\n",
    "        spark.stop()\n",
    "except:\n",
    "    pass    \n",
    "\n",
    "spark = (SparkSession.builder.appName(\"Multidimensional Data Frame\")\n",
    "        .config(\"spark.port.maxRetries\", \"200\")\n",
    "        .config(\"spark.sql.mapKeyDedupPolicy\", \"LAST_WIN\")  # This configuration allow the duplicate keys in the map data type.\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        .getOrCreate())\n",
    "\n",
    "# confiture the log level (defaulty is WARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31732f-da42-4ea4-96be-e1613066d48b",
   "metadata": {},
   "source": [
    "# Transformers and estimators: The building blocks of ML in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff469e1-c720-49b9-b616-4988de61e063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "CONTINUOUS_NB = [\"rating\", \"calories_i\", \"protein_i\", \"fat_i\", \"sodium_i\"]\n",
    " \n",
    "continuous_assembler = VectorAssembler(\n",
    "    inputCols=CONTINUOUS_NB, outputCol=\"continuous\"\n",
    ")\n",
    " \n",
    "continuous_scaler = MinMaxScaler(\n",
    "    inputCol=\"continuous\",\n",
    "    outputCol=\"continuous_scaled\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18ac52-80af-4769-b972-7f73b514e16c",
   "metadata": {},
   "source": [
    "## Data comes in, data comes out: The Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a61c8b-5702-4294-990d-67795f8ac960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(continuous_assembler.outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44172e2a-d865-4bbc-97c1-f68cc5b50a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the value of a specific param\n",
    "print(continuous_assembler.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabac73-faa6-4e57-b9ae-93dffc163bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see all the Params at once\n",
    "print(continuous_assembler.explainParam(\"outputCol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8ee51-33cd-4dc0-93aa-6f4a1892d309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see multuiple Params\n",
    "print(continuous_assembler.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf170a7-d157-4480-9eb1-d7933a402976",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SETTING PARAMS OF AN INSTANTIATED TRANSFORMER USING GETTERS AND SETTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b2bf2-d3b1-4717-849e-5b5560e00fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set param\n",
    "continuous_assembler.setOutputCol(\"more_continuous\")\n",
    "  \n",
    "print(continuous_assembler.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b20f2-aeee-4b2c-8e40-0e8d4625c334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set multiple params\n",
    "\n",
    "continuous_assembler.setParams(\n",
    "    inputCols=[\"one\", \"two\", \"three\"], handleInvalid=\"skip\"\n",
    ")\n",
    "print(continuous_assembler.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14767002-9d55-47fc-bd62-43224a106884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clear Param\n",
    "continuous_assembler.clear(continuous_assembler.handleInvalid)\n",
    " \n",
    "print(continuous_assembler.getHandleInvalid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aaaf9d-d9c1-4b05-801c-33c843f413c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data comes in, transformer comes out: The Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f2353-ed2b-4062-bb5e-9a3c2fb8ac40",
   "metadata": {},
   "source": [
    "# Building a (complete) machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ebdeea-a0c1-4323-bacb-cbb99d660ea9",
   "metadata": {},
   "source": [
    "## load processed data from Chapter 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836a461c-10cc-403f-afa3-1d7a36b4f0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "food=spark.read.parquet(root_path+'recipes/recipes_cleaned')\n",
    "\n",
    "# find all columns that only have only 1 or 1 value\n",
    "def columns_with_only_0_and_1(df):\n",
    "    return [c for c in df.columns if sorted(df.agg(F.collect_set(c)).first()[0]) == [0, 1]]\n",
    "\n",
    "BINARY_COLUMNS=columns_with_only_0_and_1(food)\n",
    "\n",
    "# remove target desset from BINARY_COLUMNS\n",
    "\n",
    "BINARY_COLUMNS.remove('dessert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b6f72-4330-40cc-b6bf-6e458c9524f9",
   "metadata": {},
   "source": [
    "## food_pipeline pipeline, containing three stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1093331f-ef26-4c0a-8aa2-892c1ccee317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.feature as MF\n",
    " \n",
    "imputer = MF.Imputer(                        \n",
    "    strategy=\"mean\",\n",
    "    inputCols=[\"rating\", \"calories\", \"protein\", \"fat\", \"sodium\"],\n",
    "    outputCols=[\"rating_i\", \"calories_i\", \"protein_i\", \"fat_i\", \"sodium_i\"],\n",
    ")\n",
    " \n",
    "continuous_assembler = MF.VectorAssembler(  \n",
    "    inputCols=[\"rating_i\", \"calories_i\", \"protein_i\", \"fat_i\", \"sodium_i\"],\n",
    "    outputCol=\"continuous\",\n",
    ")\n",
    " \n",
    "continuous_scaler = MF.MinMaxScaler(         \n",
    "     inputCol=\"continuous\",\n",
    "   outputCol=\"continuous_scaled\",\n",
    ")\n",
    " \n",
    "food_pipeline = Pipeline(                    \n",
    "     stages=[imputer, continuous_assembler, continuous_scaler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480b460-aca6-4a26-8a7b-fcd8df09352b",
   "metadata": {},
   "source": [
    "## Assembling the final data set with the vector column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453d7241-8b58-455e-97e2-122580f44123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preml_assembler = MF.VectorAssembler(\n",
    "    inputCols=BINARY_COLUMNS                         \n",
    "    + [\"continuous_scaled\"]\n",
    "    + [\"protein_ratio\", \"fat_ratio\"],\n",
    "    outputCol=\"features\",\n",
    ")\n",
    " \n",
    "food_pipeline.setStages(\n",
    "    [imputer, continuous_assembler, continuous_scaler, preml_assembler]\n",
    ")\n",
    " \n",
    "food_pipeline_model = food_pipeline.fit(food)        \n",
    "food_features = food_pipeline_model.transform(food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68fd68-1d24-4f37-9d30-ef415390d7de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "food_features.select(\"title\", \"dessert\", \"features\").show(5, truncate=30)\n",
    "\n",
    "# Note: Since we have 513 elements in our vector, with a majority of zeroes, \n",
    "# PySpark uses a sparse vector representation to save some space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d291786-ba40-4d4f-8cdb-523969c23ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chcck the labels for each feature\n",
    "#food_features.schema[\"features\"].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a9163-ae6b-4d12-9d95-2488beb0bc90",
   "metadata": {},
   "source": [
    "## Training an ML model using a LogisticRegression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9290b6f4-c34b-40b0-a5e8-12f8659a1510",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_af5dc8ed96d6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    " \n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"dessert\", predictionCol=\"prediction\"\n",
    ")\n",
    " \n",
    "food_pipeline.setStages(\n",
    "    [\n",
    "        imputer,\n",
    "        continuous_assembler,\n",
    "        continuous_scaler,\n",
    "        preml_assembler,\n",
    "        lr,\n",
    "     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7fcc80d-dd23-40c7-9ba5-c5e67f7ce4de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# split data into train and test set\n",
    "train, test = food.randomSplit([0.7, 0.3], 13)\n",
    "\n",
    "# cache train set in memory\n",
    "train.cache()\n",
    "\n",
    "# train the pipeline model using fit\n",
    "\n",
    "food_pipeline_model = food_pipeline.fit(train)\n",
    "\n",
    "# evaluate the model using transform\n",
    "results = food_pipeline_model.transform(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6382609-d74f-4c56-9686-eced9beb348e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------------------------------+------------------------------------------+\n",
      "|dessert|prediction|rawPrediction                           |probability                               |\n",
      "+-------+----------+----------------------------------------+------------------------------------------+\n",
      "|1.0    |1.0       |[-14.457074877136183,14.457074877136183]|[5.264679380571837E-7,0.999999473532062]  |\n",
      "|0.0    |0.0       |[6.616327380061065,-6.616327380061065]  |[0.9986634516178716,0.0013365483821283775]|\n",
      "|0.0    |0.0       |[0.6102071857759856,-0.6102071857759856]|[0.6479880625083543,0.3520119374916457]   |\n",
      "+-------+----------+----------------------------------------+------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check prediction result\n",
    "\n",
    "results.select(\"dessert\", \"prediction\", \"rawPrediction\", \"probability\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e73610-55b5-4bdb-8df0-c65f78719f49",
   "metadata": {},
   "source": [
    "## Evaluating and optimizing our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e00db18-dbc3-48a5-9284-84701a8be880",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assessing model accuracy: Confusion matrix and evaluator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c566d13-a5e2-4e14-99b3-fad797b30bac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2289:==================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+\n",
      "|dessert| 0.0| 1.0|\n",
      "+-------+----+----+\n",
      "|    0.0|4905| 108|\n",
      "|    1.0|  78|1013|\n",
      "+-------+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results.groupby(\"dessert\").pivot(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03566d5e-a92a-4f0d-846e-604b70aacd03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dessert=0.0, 0.0=4905, 1.0=108), Row(dessert=1.0, 0.0=78, 1.0=1013)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accouracy, precision, recall, f1 score\n",
    "\n",
    "results2=results.groupby(\"dessert\").pivot(\"prediction\").count().collect()\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8828a7d4-ba76-49e3-858a-36d3ccdd47c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9695281782437746 0.903657448706512 0.928505957836847 0.9159132007233273\n"
     ]
    }
   ],
   "source": [
    "TN=results2[0][1]\n",
    "FP=results2[0][2]\n",
    "FN=results2[1][1]\n",
    "TP=results2[1][2]\n",
    "\n",
    "accuracy=(TP+TN)/(TP+FP+TN+FN)\n",
    "precision=TP/(TP+FP)\n",
    "recall=TP/(TP+FN)\n",
    "f1=2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ff5c9ef-4a11-4b76-9e4b-c86aab19b92d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9695281782437746\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98      5013\n",
      "         1.0       0.90      0.93      0.92      1091\n",
      "\n",
      "    accuracy                           0.97      6104\n",
      "   macro avg       0.94      0.95      0.95      6104\n",
      "weighted avg       0.97      0.97      0.97      6104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use classification_report from python\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df=results.select('dessert', 'prediction').toPandas()\n",
    "label=df['dessert']\n",
    "pred=df['prediction']\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(label, pred))\n",
    "print(\"\\n\", classification_report(label, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f5fa883-ee03-440e-83ed-8583f6d9a4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model precision: 0.903657448706512\n",
      "Model recall: 0.928505957836847\n"
     ]
    }
   ],
   "source": [
    "# user the model evaluate\n",
    "lr_model = food_pipeline_model.stages[-1]                 \n",
    "metrics = lr_model.evaluate(results.select(\"title\", \"dessert\", \"features\"))\n",
    "\n",
    "# LogisticRegressionTrainingSummary\n",
    " \n",
    "print(f\"Model precision: {metrics.precisionByLabel[1]}\") \n",
    "print(f\"Model recall: {metrics.recallByLabel[1]}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb9b6b-631d-4244-9809-00eb004ff831",
   "metadata": {},
   "source": [
    "### True positives vs. false positives: The ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7524fd-24e7-4b9e-9f50-5e13a6a06d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    " \n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"dessert\",                     \n",
    "    rawPredictionCol=\"rawPrediction\",      \n",
    "    metricName=\"areaUnderROC\",\n",
    ")\n",
    " \n",
    "accuracy = evaluator.evaluate(results)\n",
    "print(f\"Area under ROC = {accuracy} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20754c1d-acb5-4364-b6c4-9d31d4719ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot roc curve\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.plot(\n",
    "    lr_model.summary.roc.select(\"FPR\").collect(),\n",
    "    lr_model.summary.roc.select(\"TPR\").collect(),\n",
    ")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5e42b-a108-43ef-8e40-32fb93f817b0",
   "metadata": {},
   "source": [
    "## Optimizing hyperparameters with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29808a9c-5345-4f4b-85d1-f5f8220e1246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using ParamGridBuilder to define a set of hyperparameters\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    " \n",
    "grid_search = (\n",
    "    ParamGridBuilder()                          \n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1.0])    \n",
    "    .build()                                    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151fd3ab-a473-4c90-a27e-70a398a5da7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating and using a CrossValidator object\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    " \n",
    "cv = CrossValidator(\n",
    "    estimator=food_pipeline,\n",
    "    estimatorParamMaps=grid_search,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=13,\n",
    "    collectSubModels=True,\n",
    ")\n",
    " \n",
    "cv_model = cv.fit(train)                     \n",
    " \n",
    "print(cv_model.avgMetrics)\n",
    " \n",
    "pipeline_food_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00f03e-5c6a-4ea4-a511-92cb4fa6d24d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting the biggest drivers from our model: Extracting the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ad967-11fd-4988-a7cc-fd4237ea8a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "feature_names = [\"(Intercept)\"] + [                   \n",
    "    x[\"name\"]\n",
    "    for x in (\n",
    "        food_features\n",
    "        .schema[\"features\"]\n",
    "        .metadata[\"ml_attr\"][\"attrs\"][\"numeric\"]\n",
    "    )\n",
    "]\n",
    "feature_coefficients = [lr_model.intercept] + list(     \n",
    "    lr_model.coefficients.values\n",
    ")\n",
    " \n",
    " \n",
    "coefficients = pd.DataFrame(\n",
    "    feature_coefficients, index=feature_names, columns=[\"coef\"]\n",
    ")\n",
    " \n",
    "coefficients[\"abs_coef\"] = coefficients[\"coef\"].abs()\n",
    " \n",
    "print(coefficients.sort_values([\"abs_coef\"], ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13113177-6a3b-49fa-9b92-8353681b9528",
   "metadata": {},
   "source": [
    "## Save the pipeline mode for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6a782-303d-4f75-ab79-5647ab29f89a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_food_model.write().overwrite().save(\"dessert_model_logisticsRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453d500-b27f-4388-9f96-72888c959306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load the model\n",
    "\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    " \n",
    "loaded_model = PipelineModel.load(\"dessert_model_logisticsRegression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a842ac-674d-48c1-b1d3-f6a0be2257d9",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Transformers are objects that, through a transform() method, modify a data frame based on a set of Params that drives its behavior. We use a transformer stage when we want to deterministically transform a data frame.\n",
    "- Estimators are objects that, through a fit() method, take a data frame and return a fully parameterized transformer called a model. We use an estimator stage when we want to transform a data frame using a data-dependent transformer.\n",
    "- ML pipelines are like estimators, as they use the fit() method to yield a pipeline model. They have a single Param, stages, that carries an ordered list of transformers and estimators to be applied on a data frame.\n",
    "- Before training a model, every feature needs to be assembled in a vector using the VectorAssembler transformer. This provides a single optimized (sparse or dense) column containing all the features for machine learning.\n",
    "- PySpark provides useful metrics for model evaluation through a set of evaluator objects. You select the appropriate one based on your type of prediction (binary classification = BinaryClassificationEvaluator).\n",
    "- With a Param Map grid, an evaluator, and an estimator, we can perform model hyperparameter optimization to try different scenarios and try to improve model accuracy.\n",
    "- Cross-validation is a technique that resamples the data frame into different partitions before fitting/testing the model. We use cross-validation to test if the model performs consistently when it sees different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69828bfc-bf7f-4ac5-8d06-30898f4f651f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Data Analytics",
   "language": "python",
   "name": "bigdataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

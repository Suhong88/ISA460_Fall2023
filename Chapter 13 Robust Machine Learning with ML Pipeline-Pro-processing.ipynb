{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9cd268-8ed1-49eb-a7a0-54cef7423ea9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98dd82f1-09e6-4cc7-8654-463f9f295a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "root_path='/net/clusterhn/home/'+account+'/isa460/Data/'\n",
    "\n",
    "# append path to helper_functions to system path\n",
    "sys.path.append('/net/clusterhn/home/sli/isa460_sli')\n",
    "\n",
    "import helper_functions as H\n",
    "\n",
    "# check if the Spark session is active. If it is activate, close it\n",
    "\n",
    "try:\n",
    "    if spark:\n",
    "        spark.stop()\n",
    "except:\n",
    "    pass    \n",
    "\n",
    "spark = (SparkSession.builder.appName(\"Multidimensional Data Frame\")\n",
    "        .config(\"spark.port.maxRetries\", \"100\")\n",
    "        .config(\"spark.sql.mapKeyDedupPolicy\", \"LAST_WIN\")  # This configuration allow the duplicate keys in the map data type.\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        .getOrCreate())\n",
    "\n",
    "# confiture the log level (defaulty is WARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c27f96-164e-43fe-9e40-e1b557840dd6",
   "metadata": {},
   "source": [
    "# Pre-processing/cleaning/transform data using the steps in Chapter 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0fd2d-2c75-4d64-9ca9-73708eb558b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "food=spark.read.csv(root_path+'recipes/epi_r.csv', inferSchema=True, header=True)\n",
    "\n",
    "# Satndardizing columns names using toDF\n",
    "def sanitize_column_name(name):\n",
    "    \"\"\"Drops unwanted characters from the column name.\n",
    " \n",
    "    We replace spaces, dashes and slashes with underscore,\n",
    "    and only keep alphanumeric characters.\n",
    "    \"\"\"\n",
    "    answer = name\n",
    "    for i, j in ((\" \", \"_\"), (\"-\", \"_\"), (\"/\", \"_\"), (\"&\", \"and\")):   \n",
    "        answer = answer.replace(i, j).lower()\n",
    "    return \"\".join(\n",
    "        [\n",
    "            char\n",
    "            for char in answer\n",
    "            if char.isalpha() or char.isdigit() or char == \"_\"       \n",
    "        ]\n",
    "    )\n",
    "# use toDF() to apply functions to all columns in the data frame\n",
    "food = food.toDF(*[sanitize_column_name(name) for name in food.columns])\n",
    "\n",
    "# for cakeweek and wasteless, only keep 0, 1 and null. Drop the rest\n",
    "\n",
    "food = food.where(\n",
    "    (\n",
    "        F.col(\"cakeweek\").isin([0.0, 1.0])    \n",
    "        | F.col(\"cakeweek\").isNull()         \n",
    "    )\n",
    "    & (\n",
    "        F.col(\"wasteless\").isin([0.0, 1.0])    \n",
    "        | F.col(\"wasteless\").isNull()         \n",
    "    )\n",
    ")\n",
    "\n",
    "# rating and calories should be double. Covert the type from string to double\n",
    "\n",
    "food=food.withColumn('rating', F.col('rating').cast('Double')) \\\n",
    "     .withColumn('calories', F.col('calories').cast('Double'))\n",
    "\n",
    "# Create four top level variables\n",
    "IDENTIFIERS = [\"title\"]\n",
    " \n",
    "CONTINUOUS_COLUMNS = [\n",
    "    \"rating\",\n",
    "    \"calories\",\n",
    "    \"protein\",\n",
    "    \"fat\",\n",
    "    \"sodium\",\n",
    "]\n",
    " \n",
    "TARGET_COLUMN = [\"dessert\"]\n",
    " \n",
    "BINARY_COLUMNS = [\n",
    "    x\n",
    "    for x in food.columns\n",
    "    if x not in CONTINUOUS_COLUMNS\n",
    "    and x not in TARGET_COLUMN\n",
    "    and x not in IDENTIFIERS\n",
    "]\n",
    "\n",
    "# drop records where all features are null\n",
    "\n",
    "food = food.dropna(\n",
    "    how=\"all\",\n",
    "    subset=[x for x in food.columns if x not in IDENTIFIERS]\n",
    ")\n",
    "\n",
    "# Deal with missing values (Weeding our useless records and imputing binary features)\n",
    "# drop records whereh target column is null\n",
    "\n",
    "food = food.dropna(subset=TARGET_COLUMN)\n",
    "\n",
    "# impute binary columns. Fill null value with 0\n",
    "\n",
    "food=food.fillna(0.0, subset=BINARY_COLUMNS)\n",
    "\n",
    "# Take care of extreme values/outliners\n",
    "\n",
    "# return 99 percentile of each feature of the following features\n",
    "\n",
    "selected_columns=[ \"calories\", \"protein\", \"fat\",\"sodium\"]\n",
    "\n",
    "maximum={}\n",
    "\n",
    "for c in selected_columns:\n",
    "    maximum[c]=food.select(F.percentile_approx(c, 0.99)).collect()[0][0]\n",
    "    \n",
    "# for the above feature, replace any value over 99 percentile to the value at 99 percentile\n",
    "\n",
    "for k, v in maximum.items():\n",
    "    food=food.withColumn(k, F.when(F.isnull(F.col(k)), F.col(k)).otherwise(\n",
    "        F.least(F.col(k), F.lit(v))\n",
    "    ))\n",
    "\n",
    "### Weeding out the rare binary occurrence columns   \n",
    "# for binary variables, remove features with less then 10 of 0 or 1.\n",
    "\n",
    "inst_sum_of_binary_columns = [\n",
    "    F.sum(F.col(x)).alias(x) for x in BINARY_COLUMNS\n",
    "]\n",
    " \n",
    "sum_of_binary_columns = (\n",
    "    food.select(*inst_sum_of_binary_columns).head().asDict()         \n",
    ")\n",
    "\n",
    "num_rows=food.count()\n",
    "too_rare_features = [\n",
    "    k\n",
    "    for k, v in sum_of_binary_columns.items()\n",
    "    if v < 10 or v > (num_rows - 10)\n",
    "]\n",
    "\n",
    "BINARY_COLUMNS = list(set(BINARY_COLUMNS) - set(too_rare_features)) \n",
    "\n",
    "# Feature Creation: protein_ratio and fat_ration\n",
    "\n",
    "food = food.withColumn(\n",
    "    \"protein_ratio\", F.col(\"protein\") * 4 / F.col(\"calories\")\n",
    ").withColumn(\n",
    "    \"fat_ratio\", F.col(\"fat\") * 9 / F.col(\"calories\")\n",
    ")                                                           \n",
    " \n",
    "food = food.fillna(0.0, subset=[\"protein_ratio\", \"fat_ratio\"])\n",
    " \n",
    "CONTINUOUS_COLUMNS += [\"protein_ratio\", \"fat_ratio\"] \n",
    "\n",
    "# Imputing continuous features using the Imputer estimator\n",
    "\n",
    "# impute the following 5 featurs with mean\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    " \n",
    "OLD_COLS = [\"rating\", \"calories\", \"protein\", \"fat\", \"sodium\"]\n",
    "NEW_COLS = [\"rating_i\", \"calories_i\", \"protein_i\", \"fat_i\", \"sodium_i\"]\n",
    " \n",
    "imputer = Imputer(\n",
    "    strategy=\"mean\",                                            \n",
    "    inputCols=OLD_COLS,                                         \n",
    "    outputCols=NEW_COLS,                                        \n",
    ")\n",
    " \n",
    "imputer_model = imputer.fit(food)  \n",
    "\n",
    "food_imputed = imputer_model.transform(food)\n",
    " \n",
    "CONTINUOUS_COLUMNS = (\n",
    "    list(set(CONTINUOUS_COLUMNS) - set(OLD_COLS)) + NEW_COLS    \n",
    ")\n",
    "\n",
    "CONTINUOUS_COLUMNS=set(CONTINUOUS_COLUMNS)\n",
    "\n",
    "# Scaling our features using the MinMaxScaler estimator\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    " \n",
    "CONTINUOUS_NB = [x for x in CONTINUOUS_COLUMNS if \"ratio\" not in x]\n",
    " \n",
    "continuous_assembler = VectorAssembler(\n",
    "    inputCols=CONTINUOUS_NB, outputCol=\"continuous\"\n",
    ")\n",
    " \n",
    "food_features = continuous_assembler.transform(food_imputed)\n",
    " \n",
    "continuous_scaler = MinMaxScaler(\n",
    "    inputCol=\"continuous\",\n",
    "    outputCol=\"continuous_scaled\",\n",
    ")\n",
    " \n",
    "food_features = continuous_scaler.fit(food_features).transform(food_features)\n",
    "\n",
    "food_features.select(\"continuous_scaled\").show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69828bfc-bf7f-4ac5-8d06-30898f4f651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the processed data to a directory\n",
    "\n",
    "#food_features.write.parquet(root_path+'recipes/recipes_cleaned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Data Analytics",
   "language": "python",
   "name": "bigdataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c0816a-b4c6-4546-8681-4420001a9b69",
   "metadata": {},
   "source": [
    "## Assignment 1: Word Count\n",
    "\n",
    "In this assignment, you will identify the most popular words appearing in the paper Detecting Covid19 Fake News. You will create a word cloud to visualize the results. You will first upload the file covid19_fake_news_detection.txt to the hpc server. Please complete the five questions at this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479e293c-af69-4b5a-9b73-65c4f243068d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in isa460 folder\n",
    "root_path='/net/clusterhn/home/'+account+'/isa460/Data/'\n",
    "\n",
    "spark = SparkSession.builder.appName(\"My First Spark Program\")\\\n",
    "        .config(\"spark.port.maxRetries\", \"100\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# confiture the log level (defaulty is WWARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# read the csv file\n",
    "\n",
    "book = spark.read.text(root_path+\"covid19_fake_news_detection.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2457b699-e642-46c1-b9d8-b95d3d1388c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/18 06:45:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from helper_functions import displayByGroup\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a spark session\n",
    "spark = SparkSession.builder.appName(\"Boston Crime Analysis\")\\\n",
    "        .config(\"spark.port.maxRetries\", \"100\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# confiture the log level (defaulty is WWARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "# read the csv file, remove year with null value and remove current month (Septmber 2023)\n",
    "crime = spark.read.csv(\"/opt/shared/boston_crime\", header=True, inferSchema=True).filter(F.isnull(F.col('year'))==False)\\\n",
    "       .filter(F.date_format(F.col('occurred_on_date'), 'yyyy-MM')!='2023-09')\n",
    "\n",
    "# change all columns to lower case\n",
    "\n",
    "crime=crime.toDF(*[x.lower() for x in crime.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4b9cf0-dc38-4ac6-8508-5c1b3643af90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "| Detecting Covid-19 Misinformation on Social Media|\n",
      "|Jason Michaud, Bryant University, jmichaud1@bry...|\n",
      "|      Suhong Li, Bryant University, sli@bryant.eud|\n",
      "|                                                  |\n",
      "|                                          Abstract|\n",
      "|                                                  |\n",
      "|There have been many studies conducted over the...|\n",
      "|                                                  |\n",
      "|Keywords: Misinformation, COVID-19, Machine Lea...|\n",
      "|                                                  |\n",
      "+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab4fe97-ce52-4892-b082-d984ec83c1a0",
   "metadata": {},
   "source": [
    "### Below code shows the top 10 words based on frequency after changing all words to lowercase, remove null value and special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f71da7ed-e1a8-46cf-89b9-9632897368ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   word|count|\n",
      "+-------+-----+\n",
      "|    the|  313|\n",
      "|    and|  159|\n",
      "|     of|  131|\n",
      "|     in|  117|\n",
      "|     is|  115|\n",
      "|      a|  114|\n",
      "|    for|  111|\n",
      "|     to|  107|\n",
      "|   news|   99|\n",
      "|   this|   91|\n",
      "|   fake|   79|\n",
      "|   that|   78|\n",
      "|  covid|   76|\n",
      "|     as|   68|\n",
      "|    was|   63|\n",
      "| united|   61|\n",
      "|     be|   55|\n",
      "|   real|   52|\n",
      "|     it|   47|\n",
      "|dataset|   46|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode,\n",
    "    lower,\n",
    "    regexp_extract,\n",
    "    split,\n",
    ")\n",
    "\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    " \n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    " \n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word\"))\n",
    " \n",
    "words_clean = words_lower.select(\n",
    "    regexp_extract(col(\"word\"), \"[a-z']*\", 0).alias(\"word\")\n",
    ")\n",
    " \n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")\n",
    " \n",
    "results = words_nonull.groupby(col(\"word\")).count()\n",
    " \n",
    "results.orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb68320-aafc-4650-93c9-a2986504d204",
   "metadata": {},
   "source": [
    "## 1. Modify the orignal code to return the number of the distinct words in this article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0de78-2970-4ba9-b499-d29eafe6e95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdb0c1cf-c094-4384-a16b-9e9486e64142",
   "metadata": {},
   "source": [
    "## 2. Modify the origianl code to return a smaple of 10 words that only appreas once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e5ee7-e3b0-4f8b-ab37-fe4f64269c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b83adc7e-4741-48d8-95d3-8ed5688b0d24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Modify original code to remove all words having less than 3 letters, show top 20 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188fe447-bc7d-494a-8761-5ff340b2bd05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cf0033b-986a-41f6-a9f8-1f45ebf52812",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Modify the above code to remove the stopwords (the, and, for, this, that, was, are) after removing words having less than 3 letters,  save the top 100 words in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032c859-38e9-4642-a52e-1380b2e8a8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b42f9c5c-c8a1-4004-85b4-ecaf2145bea1",
   "metadata": {},
   "source": [
    "## 5. Create a word cloud to visualize the top 100 words (hint: you may need to install other package, to install a package within the notebook, use \n",
    "!pip install packagename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a19a5-3134-4f63-a817-ac0590a1b438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Data Analytics",
   "language": "python",
   "name": "bigdataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b13537-88ff-433c-89f7-ce03bbf2458b",
   "metadata": {},
   "source": [
    "# Chapter 7 Bilingual PySpark: Blending Python and SQL code\n",
    "\n",
    "This chapter covers\n",
    "\n",
    "- Drawing a parallel between PySparkâ€™s instruction set and the SQL vocabulary\n",
    "- Registering data frames as temporary views or tables to query them using Spark SQL\n",
    "- Using the catalog to create, reference, and delete registered tables for SQL querying\n",
    "- Translating common data manipulations instructions from Python to SQL, and vice versa\n",
    "- Using SQL-style clauses inside certain PySpark methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09a8f6-24c9-4a9a-af83-ae75b0c54ad3",
   "metadata": {},
   "source": [
    "# Start a pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c07b2616-6954-47b0-a518-7a5e4c3f6a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import helper_functions\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change the account name to your email account\n",
    "account='sli'\n",
    "\n",
    "# define a root path to access the data in the DataAnalysisWithPythonAndPySpark\n",
    "root_path='/net/clusterhn/home/'+account+'/isa460/Data/'\n",
    "\n",
    "# check if the Spark session is active. If it is activate, close it\n",
    "\n",
    "try:\n",
    "    if spark:\n",
    "        spark.stop()\n",
    "except:\n",
    "    pass    \n",
    "\n",
    "spark = (SparkSession.builder.appName(\"Multidimensional Data Frame\")\n",
    "        .config(\"spark.port.maxRetries\", \"100\")\n",
    "        .getOrCreate())\n",
    "\n",
    "# confiture the log level (defaulty is WARN)\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3184c-00af-48ff-ae7f-abb8f0a19787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper_functions import displayByGroup\n",
    "\n",
    "elements = spark.read.csv(\n",
    "    root_path+\"elements/Periodic_Table_Of_Elements.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True)\n",
    "\n",
    "displayByGroup(elements, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6bc73-d199-4634-b322-76309246ff86",
   "metadata": {},
   "source": [
    "# Register a data frame as a SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bdebbe-ccde-47d6-9116-7ae68c934e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "elements.createOrReplaceTempView(\"elements_t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee242f81-e8ef-4da6-b77c-9d029b3ea842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql=\"select period, count(*) from elements_t where phase='liq' group by period\"\n",
    "spark.sql(sql).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214ffae-251b-4ea3-a8d6-85ae0dab70d9",
   "metadata": {},
   "source": [
    "## Using the Spark catalog to manage SQL table/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81adb2d-a9bd-4953-9047-9099c40cf8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63446b-fad6-42aa-9742-fda0c27a1173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list all sql tables\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db336755-9cde-4c8b-a256-089a86436858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop sql table/view\n",
    "spark.catalog.dropTempView(\"elements_t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab763d4-9433-44e4-8f54-7922026595f5",
   "metadata": {},
   "source": [
    "# Using SQL-style expressions in PySpark\n",
    "\n",
    "we will use a public data set provided by Backblaze, which provided hard-drive data and statistics. Backblaze is a company that provides cloud storage and backup. Since 2013, they have provided data on the drives in their data center, and over time have moved to a focus on failures and diagnosis. To get the files, you can download them from the website (http://mng.bz/4jZa). We will use the latest data in June 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8098a-d008-4add-b1ec-bda3ec2ddc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('/opt/shared/backblaze/', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4e1ac-a5ee-4ac9-8c36-535756da7e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82658b6a-7265-47f3-b5f9-3e2091ca2c97",
   "metadata": {},
   "source": [
    "## Identify the model having the highest failure rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e32c19-4c53-4d2d-b274-e7b7cc6d96e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select only the useful columns for our query\n",
    "selected_columns=['date',\n",
    " 'serial_number',\n",
    " 'model',\n",
    " 'capacity_bytes',\n",
    " 'failure']\n",
    "\n",
    "\n",
    "df1=df.select(selected_columns)\n",
    "\n",
    "df1.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc8340-ceba-4b23-957d-db66d96d362e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get driver capacity in gigabytes\n",
    "\n",
    "df2=df1.selectExpr(\"model\", \"capacity_bytes/power(1024,3) capacity_GB\", \"date\", \"failure\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a2c55-b738-4dd2-8077-b11296ec6eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calcualte failure rate by model\n",
    "\n",
    "df3=df2.groupBy('model', 'capacity_GB', 'failure').count().orderBy('model', 'capacity_GB')\n",
    "\n",
    "df4=df3.groupBy('model', 'capacity_GB').pivot('failure').sum('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b858752-5c0a-4bea-b965-067a7246909e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df5=df4.filter(F.col('1').isNotNull()).withColumn('failure_rate', F.col('1')/(F.col('0')+F.col('1'))).orderBy(F.desc('failure_rate'))\n",
    "\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3eae2-7ce1-4d36-8c40-7f6ca86a1791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# is there a correlation between capacity and failure_rate\n",
    "\n",
    "df5.select(F.corr('capacity_GB', 'failure_rate')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec07a0b-ed0c-468a-b7a4-e9e07d901ba8",
   "metadata": {},
   "source": [
    "## SQL and Pyspark code comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8f1c0-9d90-496d-bae7-4ca2e62654e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=spark.read.csv('/opt/shared/backblaze/', header=True, inferSchema=True)\n",
    "\n",
    "selected_columns=['date',\n",
    " 'serial_number',\n",
    " 'model',\n",
    " 'capacity_bytes',\n",
    " 'failure']\n",
    "\n",
    "\n",
    "backblaze=df.select(selected_columns)\n",
    "\n",
    "backblaze.createOrReplaceTempView(\"backblaze_t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212178b9-4e4f-4c54-9408-7f0ce2f47fd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### display the top 3 models that have the highest max capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b7d25-f0c6-4041-aade-a78cb96d4fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SQL way\n",
    "spark.sql(\n",
    "    \"\"\"SELECT\n",
    "           model,\n",
    "           min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "           max(capacity_bytes/ pow(1024, 3)) max_GB\n",
    "        FROM backblaze_t\n",
    "        GROUP BY 1\n",
    "        ORDER BY 3 DESC\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705813e9-38a0-4fd2-90fe-01863d142123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pyspark way\n",
    "\n",
    "backblaze.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").orderBy(F.col(\"max_GB\"), ascending=False).show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d39ee8-4db8-4687-8240-09c7927c963c",
   "metadata": {},
   "source": [
    "### display the top 3 models that have the highest max capacity, remove model that min capacity is equal to max capacity\n",
    "\n",
    "filtering after grouping using having"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e489a-89ad-43de-b6a5-e58f53883f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SQL way\n",
    "spark.sql(\n",
    "    \"\"\"SELECT\n",
    "           model,\n",
    "           min(capacity_bytes / pow(1024, 3)) min_GB,\n",
    "           max(capacity_bytes/ pow(1024, 3)) max_GB\n",
    "        FROM backblaze_t\n",
    "        GROUP BY 1\n",
    "        HAVING min_GB!=max_GB\n",
    "        ORDER BY 3 DESC\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aff5b2-b5a2-4056-8fa7-575d588432d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pyspark way\n",
    "\n",
    "backblaze.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\"),\n",
    ").where(F.col(\"min_GB\") != F.col(\"max_GB\")).orderBy(F.col(\"max_GB\"), ascending=False).show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae4b6a-1e1d-4b31-ba13-f7d402f64401",
   "metadata": {},
   "source": [
    "## Creating new tables/views using the CREATE keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128310bf-5730-46ca-a20d-09bb5592b909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW drive_days AS\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM backblaze_t\n",
    "        GROUP BY model\"\"\"\n",
    ")\n",
    " \n",
    "spark.sql(\n",
    "    \"\"\"CREATE OR REPLACE TEMP VIEW failures AS\n",
    "           SELECT model, count(*) AS failures\n",
    "           FROM backblaze_t\n",
    "           WHERE failure = 1\n",
    "           GROUP BY model\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccea13-be82-4d36-8e06-6cbc16b364d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914334c1-6ee8-4044-a4e9-7ac0211d9633",
   "metadata": {},
   "source": [
    "### Adding data to our table using UNION and JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db648050-c224-4f3c-a01c-81cc75ba585c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backblaze_failure=backblaze.filter(F.col('failure')==1)\n",
    "backblaze_working=backblaze.filter(F.col('failure')==0)\n",
    "\n",
    "backblaze_failure.createOrReplaceTempView('backblaze_failure_t')\n",
    "backblaze_working.createOrReplaceTempView('backblaze_working_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6ff0e-ef75-4f79-a196-e03fef512ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sql way\n",
    "spark.sql(\"\"\"\n",
    "        create or replace temp view backblaze_complete as\n",
    "        select * from backblaze_failure_t union all\n",
    "        select * from backblaze_working_t\n",
    "\"\"\")\n",
    "\n",
    "#pyspark way\n",
    "\n",
    "backblaze_complet=backblaze_failure.union(backblaze_working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ecdd0-ca3b-42f3-80f5-021f91c49c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# left join\n",
    "spark.sql(\n",
    "    \"\"\"select\n",
    "           drive_days.model,\n",
    "           drive_days,\n",
    "           failures\n",
    "    from drive_days\n",
    "    left join failures on drive_days.model = failures.model\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70204ae7-2a68-4218-8d84-937c5d521438",
   "metadata": {},
   "source": [
    "### Organizing your SQL code better through subqueries and common table expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394bbc15-6e0f-4aa7-96a0-d3e717dbb2e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from backblaze_t\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edfb44c-a3a8-436c-83e3-e8cc7baee747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding drive models with highest failure rates using subqueries\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        failures.model,\n",
    "        failures / drive_days failure_rate\n",
    "    FROM (\n",
    "        SELECT\n",
    "            model,\n",
    "            count(*) AS drive_days\n",
    "        FROM backblaze_t\n",
    "        GROUP BY model) drive_days\n",
    "    INNER JOIN (\n",
    "        SELECT\n",
    "            model,\n",
    "            count(*) AS failures\n",
    "        FROM backblaze_t\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model) failures\n",
    "    ON\n",
    "        drive_days.model = failures.model\n",
    "    ORDER BY 2 desc\n",
    "    \"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d62f5-42df-416f-8624-dd9f54b4f76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding highest failure rates using common table expressions/CTE\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH drive_days as (            \n",
    "        SELECT                       \n",
    "            model,                   \n",
    "            count(*) AS drive_days   \n",
    "        FROM backblaze_t            \n",
    "        GROUP BY model),             \n",
    "    failures as (                    \n",
    "        SELECT                      \n",
    "            model,                   \n",
    "            count(*) AS failures    \n",
    "        FROM backblaze_t            \n",
    "        WHERE failure = 1           \n",
    "        GROUP BY model)              \n",
    "    SELECT\n",
    "        failures.model,\n",
    "        failures / drive_days failure_rate\n",
    "    FROM drive_days\n",
    "    INNER JOIN failures\n",
    "    ON\n",
    "        drive_days.model = failures.model\n",
    "    ORDER BY 2 desc\n",
    "    \"\"\"\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc0540-856a-4eed-85be-9ba1171f7321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding the highest failure rate using Python scope rules\n",
    "\n",
    "def failure_rate(df):\n",
    "    drive_days = df.groupby(F.col(\"model\")).agg(   \n",
    "        F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    "    )\n",
    "    failures = (\n",
    "       df.where(F.col(\"failure\") == 1)\n",
    "        .groupby(F.col(\"model\"))\n",
    "        .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    "    )\n",
    "    answer = (                                               \n",
    "        drive_days.join(failures, on=\"model\", how=\"inner\")\n",
    "        .withColumn(\"failure_rate\", F.col(\"failures\") / F.col(\"drive_days\"))\n",
    "        .orderBy(F.col(\"failure_rate\").desc())\n",
    "    )\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df5c27-72e6-4927-a59a-8d51096cbe11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "failure_rate(backblaze).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988802ff-f73b-48c5-9404-16e1a8b10fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_reliable_drive_for_capacity(data, capacity_GB=2048, precision=0.25, top_n=3):\n",
    "    \"\"\"Returns the top 3 drives for a given approximate capacity.\n",
    " \n",
    "    Given a capacity in GB and a precision as a decimal number, we keep the N\n",
    "    drives where:\n",
    " \n",
    "    - the capacity is between (capacity * 1/(1+precision)), capacity * (1+precision)\n",
    "    - the failure rate is the lowest\n",
    " \n",
    "    \"\"\"\n",
    "    capacity_min = capacity_GB / (1 + precision)\n",
    "    capacity_max = capacity_GB * (1 + precision)\n",
    " \n",
    "    answer = (\n",
    "        data.where(f\"capacity_GB between {capacity_min} and {capacity_max}\")\n",
    "        .orderBy(\"failure_rate\", \"capacity_GB\", ascending=[True, False])\n",
    "        .limit(top_n)                                                     \n",
    "     )\n",
    " \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cdffe4-4af2-4eac-ab9d-5024f001b3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backblaze1=backblaze.withColumn('capacity_GB', F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3))\n",
    "\n",
    "most_reliable_drive_for_capacity(df5, capacity_GB=11176.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b87e86-1c5f-437e-ada2-41b9e3999986",
   "metadata": {},
   "source": [
    "#Summary\n",
    "\n",
    "- Spark provides a SQL API for data manipulation. This API supports ANSI SQL.\n",
    "- Spark (and PySpark, by extension) borrows a lot of vocabulary and expected functionality from the way SQL manipulates tables. This is especially evident since the data manipulation module is called pyspark.sql.\n",
    "- PySparkâ€™s data frames need to be registered as views or tables before they can be queried with Spark SQL. You can give them a different name than the data frame youâ€™re registering.\n",
    "- PySparkâ€™s own data frame manipulation methods and functions borrow SQL functionality, for the most part. Some exceptions, such as union(), are present and documented in the API.\n",
    "- Spark SQL queries can be inserted in a PySpark program through the spark.sql function, where spark is the running SparkSession.\n",
    "- Spark SQL table references are kept in a Catalog, which contains the metadata for all tables accessible to Spark SQL.\n",
    "- PySpark will accept SQL-style clauses in where(), expr(), and selectExpr(), which can simplify the syntax for complex filtering and selection.\n",
    "- When using Spark SQL queries with user-provided input, be careful with sanitizing the inputs to avoid potential SQL injection attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ea7a1-9045-47cb-bdca-5c7c9dbcd9c3",
   "metadata": {},
   "source": [
    "## In class exercise\n",
    "\n",
    "display average failur rate of driver by day, order by day, and visualize the result.\n",
    "\n",
    "Use both SQL and pyspark to get the answer. Which one you prefer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b9ddb4d-fe9c-4246-9b18-1ca4659abf87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('/opt/shared/backblaze/', header=True, inferSchema=True)\n",
    "\n",
    "selected_columns=['date',\n",
    " 'serial_number',\n",
    " 'model',\n",
    " 'capacity_bytes',\n",
    " 'failure']\n",
    "\n",
    "\n",
    "backblaze=df.select(selected_columns)\n",
    "\n",
    "backblaze.createOrReplaceTempView(\"backblaze_t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0ec48-8113-4103-ac71-7a3109708c24",
   "metadata": {},
   "source": [
    "## PySpark way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808dd5d3-06a3-42fe-8473-acc359b74fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7a01f-d4e5-4415-aa16-ceb84419137b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2127749c-4221-4018-a286-a1ad7f870d7f",
   "metadata": {},
   "source": [
    "### SQL way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6f9041-f065-45c1-8d52-55290c01154b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7a721-4945-4ad1-8828-a5cd0a5c14be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4985809e-f967-44a8-be13-84899752cb3c",
   "metadata": {},
   "source": [
    "### Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe2dce-0640-46c1-98a3-f6b3452c4c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d94c5-6dee-484f-a84e-dae5c302bbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0151671-4aeb-418a-be5f-b137380914ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Data Analytics",
   "language": "python",
   "name": "bigdataanalytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
